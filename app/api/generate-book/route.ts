import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// CONFIGURA√á√ÉO REALISTA PARA 4096 TOKENS M√ÅXIMO
const BOOK_SIZE_CONFIG = {
  pequeno: {
    label: "Pequeno",
    description: "Conto ou novela curta",
    pages: "20-30 p√°ginas",
    chapters: 3,
    wordsPerChapter: "400-600 palavras",
    maxTokens: 2000, // SEGURO para 4K
    readingTime: "30-60 minutos"
  },
  medio: {
    label: "M√©dio", 
    description: "Romance curto",
    pages: "30-50 p√°ginas",
    chapters: 4,
    wordsPerChapter: "500-800 palavras",
    maxTokens: 3000, // BOM para 4K
    readingTime: "1-2 horas"
  },
  grande: {
    label: "Grande",
    description: "Romance m√©dio",
    pages: "40-60 p√°ginas",
    chapters: 5,
    wordsPerChapter: "600-900 palavras", 
    maxTokens: 3500, // M√ÅXIMO SEGURO para 4K
    readingTime: "2-3 horas"
  }
};

// Fun√ß√£o para testar modelo com limite REAL
async function testModelWithRealLimit(model: string): Promise<{available: boolean, maxTokens: number}> {
  console.log(`üîç Testando modelo: ${model}`);
  
  try {
    // Teste com limite conservador primeiro
    const testCompletion = await openai.chat.completions.create({
      model: model,
      messages: [{ role: "user", content: "Responda 'OK'" }],
      max_tokens: 5,
    });
    
    console.log(`‚úÖ ${model} dispon√≠vel`);
    
    // DEFINIR LIMITE M√ÅXIMO REAL baseado no modelo
    let maxTokens;
    if (model.includes('16k') || model.includes('32k')) {
      maxTokens = 4096; // SEU PROJETO S√ì PERMITE 4K MESMO EM MODELOS 16K!
    } else if (model.includes('gpt-4')) {
      maxTokens = 4096; // GPT-4 tamb√©m tem limite de 4K no seu projeto
    } else {
      maxTokens = 4096; // M√ÅXIMO ABSOLUTO para seu projeto
    }
    
    // Limitar ainda mais para seguran√ßa
    const safeMaxTokens = Math.min(maxTokens, 3500);
    
    console.log(`   ‚Ä¢ Limite real: ${maxTokens} tokens`);
    console.log(`   ‚Ä¢ Limite seguro: ${safeMaxTokens} tokens`);
    
    return { available: true, maxTokens: safeMaxTokens };
    
  } catch (error: any) {
    console.log(`‚ùå ${model} n√£o dispon√≠vel: ${error.message}`);
    return { available: false, maxTokens: 0 };
  }
}

// Fun√ß√£o para descobrir modelo funcionando
async function findWorkingModel(): Promise<{model: string, maxTokens: number}> {
  console.log('üöÄ Procurando modelo funcionando...');
  
  const modelsToTry = [
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-16k", 
    "gpt-4",
    "gpt-4-turbo"
  ];
  
  for (const model of modelsToTry) {
    const result = await testModelWithRealLimit(model);
    if (result.available) {
      return { model, maxTokens: result.maxTokens };
    }
  }
  
  throw new Error('Nenhum modelo GPT est√° dispon√≠vel. Verifique as permiss√µes do projeto.');
}

export async function POST(request: NextRequest) {
  console.log('üéØ INICIANDO GERA√á√ÉO COM LIMITE REAL DE 4K TOKENS');
  
  try {
    const { description, size, genre, audience, chapterCount } = await request.json();
    
    // Encontrar modelo que funciona
    const { model, maxTokens } = await findWorkingModel();
    
    console.log(`‚úÖ MODELO SELECIONADO: ${model}`);
    console.log(`üîê LIMITE M√ÅXIMO: ${maxTokens} tokens`);
    
    const config = BOOK_SIZE_CONFIG[size as keyof typeof BOOK_SIZE_CONFIG] || BOOK_SIZE_CONFIG.medio;
    const finalChapterCount = Math.min(chapterCount || config.chapters, 5);
    
    console.log(`üìä CONFIG: ${config.pages} | ${finalChapterCount} cap√≠tulos | ${maxTokens} tokens`);
    
    // Prompt ULTRA OTIMIZADO para 4K tokens
    const ultraOptimizedPrompt = `
CRIE UM LIVRO baseado nesta descri√ß√£o:
"${description.substring(0, 800)}"

CONFIGURA√á√ÉO:
- Cap√≠tulos: ${finalChapterCount}
- G√™nero: ${genre || "Fic√ß√£o"}
- P√∫blico: ${audience || "Adulto"}

INSTRU√á√ïES (SEJA CONCISO MAS DETALHADO):
- Cada cap√≠tulo: 2-3 par√°grafos substanciais
- Inclua di√°logos breves mas significativos
- Desenvolva a trama progressivamente
- Mantenha coer√™ncia com a premissa

FORMATO (APENAS JSON):
{
  "title": "T√≠tulo",
  "synopsis": "Sinopse curta de 1-2 par√°grafos",
  "chapters": [
    {
      "title": "T√≠tulo Cap√≠tulo 1", 
      "content": "Conte√∫do com 2-3 par√°grafos ricos em detalhes..."
    }
  ]
}

IMPORTANTE: Otimize o uso de tokens! Seja detalhado mas eficiente.
`;

    console.log('üìù Gerando livro otimizado...');
    
    const completion = await openai.chat.completions.create({
      model: model,
      messages: [
        {
          role: "system",
          content: "Voc√™ √© um escritor que cria conte√∫do rico dentro de limites rigorosos de tokens. Seja detalhado mas conciso."
        },
        {
          role: "user",
          content: ultraOptimizedPrompt
        }
      ],
      max_tokens: maxTokens,
      temperature: 0.7,
    });

    const content = completion.choices[0]?.message?.content;
    
    if (!content) {
      throw new Error('Resposta vazia da OpenAI');
    }

    console.log('‚úÖ Conte√∫do recebido');
    
    // Processar resposta
    let bookData;
    try {
      bookData = JSON.parse(content);
    } catch (e) {
      console.log('‚ö†Ô∏è Criando estrutura fallback...');
      bookData = {
        title: "Livro Gerado",
        synopsis: `Baseado na premissa: ${description.substring(0, 100)}...`,
        chapters: Array.from({ length: finalChapterCount }, (_, i) => ({
          title: `Cap√≠tulo ${i + 1}`,
          content: `Desenvolvimento da narrativa no cap√≠tulo ${i + 1}. ${description.substring(0, 80)}...`
        }))
      };
    }

    // Estat√≠sticas
    const totalContentLength = bookData.chapters?.reduce((sum: number, chapter: any) => 
      sum + (chapter.content?.length || 0), 0) || 0;

    console.log('üìà ESTAT√çSTICAS FINAIS:');
    console.log(`   ‚Ä¢ Modelo: ${model}`);
    console.log(`   ‚Ä¢ Tokens usados: ${completion.usage?.total_tokens}`);
    console.log(`   ‚Ä¢ Cap√≠tulos: ${bookData.chapters?.length}`);
    console.log(`   ‚Ä¢ Caracteres: ${totalContentLength}`);
    console.log(`   ‚Ä¢ P√°ginas: ~${Math.ceil(totalContentLength / 1500)}`);

    return NextResponse.json(bookData);

  } catch (error: any) {
    console.error('üí• ERRO:', error.message);
    
    if (error?.status === 400 && error?.message?.includes('max_tokens')) {
      return NextResponse.json(
        { 
          error: 'Limite de tokens excedido.',
          solution: 'Seu projeto OpenAI tem limite m√°ximo de 4096 tokens. Use tamanhos "Pequeno" ou "M√©dio".'
        },
        { status: 400 }
      );
    }
    
    if (error?.status === 403) {
      return NextResponse.json(
        { 
          error: 'Acesso negado aos modelos GPT.',
          solution: 'Seu projeto est√° restrito. Crie uma nova API Key ou verifique permiss√µes.'
        },
        { status: 403 }
      );
    }

    return NextResponse.json(
      { error: 'Erro na gera√ß√£o. Tente tamanho "Pequeno".' },
      { status: 500 }
    );
  }
}
