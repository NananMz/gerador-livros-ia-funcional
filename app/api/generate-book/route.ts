import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// CONFIGURA√á√ÉO REALISTA PARA GPT-3.5-TURBO PADR√ÉO (4K TOKENS)
const BOOK_SIZE_CONFIG = {
  pequeno: {
    label: "Pequeno",
    description: "Novela curta",
    pages: "30-50 p√°ginas",
    chapters: 3,
    wordsPerChapter: "600-900 palavras",
    estimatedWords: "2.000-3.000 palavras totais",
    maxTokens: 2500, // SEGURO para 4K
    model: "gpt-3.5-turbo",
    readingTime: "1-2 horas",
    target: "Leitura r√°pida"
  },
  medio: {
    label: "M√©dio", 
    description: "Romance padr√£o",
    pages: "50-80 p√°ginas",
    chapters: 5,
    wordsPerChapter: "800-1.200 palavras",
    estimatedWords: "4.000-6.000 palavras totais",
    maxTokens: 3500, // M√ÅXIMO SEGURO para 4K
    model: "gpt-3.5-turbo",
    readingTime: "2-3 horas",
    target: "Leitura com desenvolvimento"
  },
  grande: {
    label: "Grande",
    description: "Romance extenso",
    pages: "70-100 p√°ginas", // REDUZIDO para realidade
    chapters: 6,
    wordsPerChapter: "1.000-1.500 palavras",
    estimatedWords: "6.000-9.000 palavras totais",
    maxTokens: 3800, // NO LIMITE do seguro
    model: "gpt-3.5-turbo",
    readingTime: "3-4 horas",
    target: "Leitura detalhada"
  }
};

// Fun√ß√£o para gerar livro em partes (para tamanho grande)
async function generateBookInParts(description: string, config: any, openai: OpenAI) {
  console.log(`üìö Gerando livro grande em ${config.chapters} partes...`);
  
  const chapters = [];
  const descriptionSummary = description.substring(0, 1500); // Resumo para caber nos tokens
  
  for (let i = 0; i < config.chapters; i++) {
    console.log(`üîÑ Gerando cap√≠tulo ${i + 1} de ${config.chapters}...`);
    
    const chapterPrompt = `
## GERA√á√ÉO DE CAP√çTULO ${i + 1} para LIVRO LONGO

**PREMISSA COMPLETA:**
${descriptionSummary}

**CAP√çTULO ${i + 1} de ${config.chapters}**

Desenvolva APENAS este cap√≠tulo com:
- 800-1200 palavras
- Di√°logos realistas
- Descri√ß√µes detalhadas
- Progress√£o da trama

**FOCO DESTE CAP√çTULO:**
${getChapterFocus(i, config.chapters)}

**INSTRU√á√ïES:**
- Seja detalhado mas eficiente em tokens
- Desenvolva personagens e conflitos
- Mantenha coer√™ncia com a premissa

**FORMATO (APENAS JSON):**
{
  "title": "T√≠tulo do Cap√≠tulo ${i + 1}",
  "content": "Conte√∫do completo aqui..."
}
`;

    try {
      const completion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content: "Voc√™ √© um escritor especializado em criar cap√≠tulos individuais ricos em detalhes."
          },
          {
            role: "user",
            content: chapterPrompt
          }
        ],
        max_tokens: 1200, // Por cap√≠tulo
        temperature: 0.7,
      });

      const content = completion.choices[0]?.message?.content;
      
      if (content) {
        try {
          const chapterData = JSON.parse(content);
          chapters.push(chapterData);
          console.log(`‚úÖ Cap√≠tulo ${i + 1} gerado: ${chapterData.title}`);
        } catch (e) {
          // Fallback se JSON falhar
          chapters.push({
            title: `Cap√≠tulo ${i + 1}`,
            content: content
          });
        }
      }
      
      // Pequena pausa entre cap√≠tulos
      if (i < config.chapters - 1) {
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
      
    } catch (error) {
      console.error(`‚ùå Erro no cap√≠tulo ${i + 1}:`, error);
      chapters.push({
        title: `Cap√≠tulo ${i + 1}`,
        content: `Conte√∫do do cap√≠tulo ${i + 1} em desenvolvimento.`
      });
    }
  }
  
  return chapters;
}

function getChapterFocus(chapterIndex: number, totalChapters: number): string {
  const focuses = [
    "Introdu√ß√£o dos personagens e conflito inicial",
    "Desenvolvimento das rela√ß√µes e primeiros desafios", 
    "Aumento da tens√£o e revela√ß√µes importantes",
    "Ponto de virada e conflito central",
    "Desenvolvimento do cl√≠max",
    "Resolu√ß√£o e conclus√µes finais"
  ];
  
  return focuses[chapterIndex] || "Desenvolvimento da narrativa principal";
}

export async function POST(request: NextRequest) {
  const startTime = Date.now();
  console.log('üöÄ INICIANDO GERA√á√ÉO COM GPT-3.5-TURBO PADR√ÉO');
  
  try {
    const { description, size, genre, audience, chapterCount } = await request.json();
    
    console.log('üìã PAR√ÇMETROS:');
    console.log(`   ‚Ä¢ Tamanho: ${size}`);
    console.log(`   ‚Ä¢ G√™nero: ${genre}`);
    console.log(`   ‚Ä¢ Cap√≠tulos: ${chapterCount}`);

    // Valida√ß√µes
    if (!description || description.length < 20) {
      return NextResponse.json(
        { error: 'Descri√ß√£o muito curta (m√≠nimo 20 caracteres)' },
        { status: 400 }
      );
    }

    const config = BOOK_SIZE_CONFIG[size as keyof typeof BOOK_SIZE_CONFIG] || BOOK_SIZE_CONFIG.medio;
    const finalChapterCount = chapterCount || config.chapters;

    console.log(`üéØ CONFIGURA√á√ÉO: ${config.pages} | ${finalChapterCount} cap√≠tulos | ${config.maxTokens} tokens`);

    // Para livros grandes, usar gera√ß√£o em partes
    if (size === 'grande' && finalChapterCount > 4) {
      console.log('üîÑ Usando gera√ß√£o em partes para otimizar tokens...');
      
      const chapters = await generateBookInParts(description, {
        chapters: finalChapterCount,
        maxTokens: config.maxTokens
      }, openai);

      const bookData = {
        title: "Livro Gerado - Edi√ß√£o Completa",
        synopsis: `Obra completa desenvolvida em ${finalChapterCount} cap√≠tulos, baseada na premissa original.`,
        chapters: chapters,
        metadata: {
          estimatedPages: config.pages,
          totalChapters: chapters.length,
          generation: "multi-part-optimized",
          model: "gpt-3.5-turbo",
          maxTokens: config.maxTokens
        }
      };

      console.log(`‚úÖ LIVRO COMPLETO GERADO: ${chapters.length} cap√≠tulos`);
      return NextResponse.json(bookData);
    }

    // Para livros pequenos/m√©dios, gera√ß√£o √∫nica otimizada
    const optimizedPrompt = `
# CRIA√á√ÉO DE LIVRO OTIMIZADO (LIMITE 4K TOKENS)

## PREMISSA ORIGINAL:
${description.substring(0, 1200)}

## CONFIGURA√á√ÉO:
- CAP√çTULOS: ${finalChapterCount}
- P√ÅGINAS: ${config.pages}
- TOKENS DISPON√çVEIS: ${config.maxTokens}

## INSTRU√á√ïES:
Desenvolva cada cap√≠tulo sendo:
- ‚úÖ DETALHADO: Com di√°logos e descri√ß√µes
- ‚úÖ EFICIENTE: Otimizado para limite de tokens  
- ‚úÖ COERENTE: Seguindo a premissa original
- ‚úÖ COMPLETO: Narrativa satisfat√≥ria

**DICA:** Use descri√ß√µes ricas mas concisas. Desenvolva di√°logos significativos.

FORMATO (APENAS JSON):
{
  "title": "T√≠tulo Criativo",
  "synopsis": "Sinopse envolvente de 2-3 par√°grafos",
  "chapters": [
    {
      "title": "T√≠tulo Cap√≠tulo 1",
      "content": "Conte√∫do rico em detalhes mas eficiente em uso de tokens..."
    }
  ]
}
`;

    console.log('üöÄ Gerando livro em chamada √∫nica otimizada...');

    const completion = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "system",
          content: "Voc√™ √© um escritor que cria conte√∫do rico e detalhado dentro de limites de tokens. Seja criativo mas eficiente."
        },
        {
          role: "user",
          content: optimizedPrompt
        }
      ],
      max_tokens: config.maxTokens,
      temperature: 0.7,
    });

    const content = completion.choices[0]?.message?.content;
    
    if (!content) {
      throw new Error('Resposta vazia da OpenAI');
    }

    let bookData;
    try {
      bookData = JSON.parse(content);
    } catch (e) {
      console.log('‚ùå Erro no parse JSON, criando estrutura alternativa...');
      bookData = {
        title: "Livro Gerado - " + description.substring(0, 30),
        synopsis: `Narrativa completa baseada na premissa original.`,
        chapters: Array.from({ length: finalChapterCount }, (_, i) => ({
          title: `Cap√≠tulo ${i + 1}`,
          content: `Conte√∫do desenvolvido para o cap√≠tulo ${i + 1}, expandindo a premissa original.`
        }))
      };
    }

    // Estat√≠sticas
    const totalContentLength = bookData.chapters?.reduce((sum: number, chapter: any) => 
      sum + (chapter.content?.length || 0), 0) || 0;

    const estimatedPages = Math.ceil(totalContentLength / 1800);

    console.log('üìà ESTAT√çSTICAS:');
    console.log(`   ‚Ä¢ T√≠tulo: ${bookData.title}`);
    console.log(`   ‚Ä¢ P√°ginas: ~${estimatedPages}`);
    console.log(`   ‚Ä¢ Cap√≠tulos: ${bookData.chapters?.length}`);
    console.log(`   ‚Ä¢ Caracteres: ${totalContentLength}`);
    console.log(`   ‚Ä¢ Tokens usados: ${completion.usage?.total_tokens}`);

    // Metadados
    bookData.metadata = {
      model: "gpt-3.5-turbo",
      estimatedPages: estimatedPages,
      totalChapters: bookData.chapters?.length,
      size: config.label,
      tokensUsed: completion.usage?.total_tokens,
      maxTokensConfig: config.maxTokens
    };

    const totalTime = Date.now() - startTime;
    console.log(`üéâ GERA√á√ÉO CONCLU√çDA em ${totalTime}ms`);

    return NextResponse.json(bookData);

  } catch (error: any) {
    console.error('üí• ERRO:', error.message);
    
    if (error?.status === 400 && error?.message?.includes('max_tokens')) {
      return NextResponse.json(
        { 
          error: 'Limite de tokens excedido.',
          solution: 'Use tamanho "Pequeno" ou "M√©dio" para melhor resultado.'
        },
        { status: 400 }
      );
    }

    return NextResponse.json(
      { error: 'Erro na gera√ß√£o. Tente novamente.' },
      { status: 500 }
    );
  }
}
