import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Sistema para descobrir modelos dispon√≠veis
const AVAILABLE_MODELS = [
  "gpt-3.5-turbo",
  "gpt-3.5-turbo-16k", 
  "gpt-4",
  "gpt-4-turbo",
  "gpt-4-32k"
];

// Configura√ß√£o base independente do modelo
const BOOK_SIZE_CONFIG = {
  pequeno: {
    label: "Pequeno",
    pages: "20-40 p√°ginas",
    chapters: 3,
    wordsPerChapter: "500-800 palavras",
    readingTime: "1-2 horas"
  },
  medio: {
    label: "M√©dio", 
    pages: "40-70 p√°ginas",
    chapters: 5,
    wordsPerChapter: "700-1.000 palavras",
    readingTime: "2-3 horas"
  },
  grande: {
    label: "Grande",
    pages: "60-90 p√°ginas",
    chapters: 6,
    wordsPerChapter: "800-1.200 palavras",
    readingTime: "3-4 horas"
  }
};

// Fun√ß√£o para descobrir qual modelo est√° dispon√≠vel
async function discoverAvailableModel(): Promise<{model: string, maxTokens: number}> {
  console.log('üîç Descobrindo modelos dispon√≠veis...');
  
  for (const model of AVAILABLE_MODELS) {
    try {
      console.log(`   Testando modelo: ${model}`);
      
      const testCompletion = await openai.chat.completions.create({
        model: model,
        messages: [{ role: "user", content: "Responda 'OK'" }],
        max_tokens: 5,
      });
      
      // Determinar max_tokens baseado no modelo
      let maxTokens = 2000; // padr√£o conservador
      
      if (model.includes('16k') || model.includes('32k')) {
        maxTokens = 8000;
      } else if (model.includes('gpt-4')) {
        maxTokens = 4000;
      } else if (model.includes('gpt-3.5-turbo')) {
        maxTokens = 3500; // M√°ximo seguro para 4K
      }
      
      console.log(`‚úÖ Modelo dispon√≠vel: ${model} (${maxTokens} tokens)`);
      return { model, maxTokens };
      
    } catch (error: any) {
      console.log(`‚ùå ${model} n√£o dispon√≠vel: ${error.message}`);
      continue;
    }
  }
  
  // Se nenhum modelo GPT funcionar, tentar usar davinci como fallback
  try {
    console.log('üîÑ Tentando modelos completion como fallback...');
    const completion = await openai.completions.create({
      model: "text-davinci-003",
      prompt: "Test",
      max_tokens: 5,
    });
    
    console.log('‚úÖ Usando text-davinci-003 como fallback');
    return { model: "text-davinci-003", maxTokens: 2000 };
    
  } catch (error) {
    throw new Error('NENHUM modelo da OpenAI est√° dispon√≠vel para este projeto');
  }
}

// Fun√ß√£o para gerar livro com o modelo dispon√≠vel
async function generateWithAvailableModel(prompt: string, model: string, maxTokens: number) {
  console.log(`üöÄ Gerando com ${model} (${maxTokens} tokens)...`);
  
  if (model.startsWith('text-')) {
    // Para modelos de completion (antigos)
    const completion = await openai.completions.create({
      model: model,
      prompt: prompt,
      max_tokens: maxTokens,
      temperature: 0.7,
    });
    
    return completion.choices[0]?.text || '';
  } else {
    // Para modelos chat
    const completion = await openai.chat.completions.create({
      model: model,
      messages: [
        {
          role: "system",
          content: "Voc√™ √© um escritor profissional. Responda APENAS com JSON v√°lido."
        },
        {
          role: "user",
          content: prompt
        }
      ],
      max_tokens: maxTokens,
      temperature: 0.7,
    });
    
    return completion.choices[0]?.message?.content || '';
  }
}

export async function POST(request: NextRequest) {
  console.log('üöÄ INICIANDO GERA√á√ÉO COM MODELO AUTODETECTADO');
  
  try {
    const { description, size, genre, audience, chapterCount } = await request.json();
    
    // Descobrir modelo dispon√≠vel
    const { model, maxTokens } = await discoverAvailableModel();
    
    console.log(`üéØ MODELO SELECIONADO: ${model}`);
    console.log(`   ‚Ä¢ Tokens dispon√≠veis: ${maxTokens}`);
    console.log(`   ‚Ä¢ Tamanho solicitado: ${size}`);
    
    const config = BOOK_SIZE_CONFIG[size as keyof typeof BOOK_SIZE_CONFIG] || BOOK_SIZE_CONFIG.medio;
    const finalChapterCount = Math.min(chapterCount || config.chapters, 6);
    
    // Ajustar configura√ß√£o baseado nos tokens dispon√≠veis
    let adjustedConfig = { ...config };
    if (maxTokens < 3000) {
      // Tokens muito limitados
      adjustedConfig.chapters = Math.min(config.chapters, 4);
      adjustedConfig.wordsPerChapter = "400-600 palavras";
      adjustedConfig.pages = `~${parseInt(config.pages.split('-')[0]) / 2} p√°ginas`;
    }
    
    console.log(`üìä CONFIGURA√á√ÉO AJUSTADA: ${adjustedConfig.pages} | ${finalChapterCount} cap√≠tulos`);
    
    // Prompt otimizado para o modelo dispon√≠vel
    const optimizedPrompt = model.startsWith('text-') 
      ? // Prompt para modelos de completion
        `Crie um livro baseado em: "${description.substring(0, 1000)}"
        
        G√™nero: ${genre}
        Cap√≠tulos: ${finalChapterCount}
        
        Estruture em JSON:
        {
          "title": "T√≠tulo",
          "synopsis": "Sinopse", 
          "chapters": [
            {"title": "Cap√≠tulo 1", "content": "Texto"}
          ]
        }`
      : // Prompt para modelos chat
        `Crie um livro completo baseado nesta descri√ß√£o: "${description.substring(0, 1500)}"
        
        ESPECIFICA√á√ïES:
        - G√™nero: ${genre || "Fic√ß√£o"}
        - Cap√≠tulos: ${finalChapterCount}
        - P√∫blico: ${audience || "Adulto"}
        
        Desenvolva cada cap√≠tulo com:
        - Di√°logos realistas
        - Descri√ß√µes detalhadas  
        - Progress√£o narrativa
        - Desenvolvimento de personagens
        
        FORMATO DE RESPOSTA (APENAS JSON):
        {
          "title": "T√≠tulo do Livro",
          "synopsis": "Sinopse completa aqui",
          "chapters": [
            {
              "title": "T√≠tulo do Cap√≠tulo 1",
              "content": "Conte√∫do completo e detalhado do cap√≠tulo 1..."
            }
          ]
        }`;

    console.log('üìù Gerando conte√∫do...');
    
    const content = await generateWithAvailableModel(optimizedPrompt, model, maxTokens);
    
    if (!content) {
      throw new Error('Resposta vazia da OpenAI');
    }
    
    console.log('‚úÖ Conte√∫do recebido, processando...');
    
    // Processar resposta
    let bookData;
    try {
      bookData = JSON.parse(content);
    } catch (e) {
      console.log('‚ùå Erro no parse JSON, criando estrutura manual...');
      
      // Se n√£o √© JSON v√°lido, criar estrutura b√°sica
      const chapters = [];
      for (let i = 0; i < finalChapterCount; i++) {
        chapters.push({
          title: `Cap√≠tulo ${i + 1}`,
          content: `Conte√∫do do cap√≠tulo ${i + 1} baseado na descri√ß√£o: ${description.substring(0, 100)}...`
        });
      }
      
      bookData = {
        title: "Livro Gerado",
        synopsis: `Baseado na premissa: ${description.substring(0, 200)}...`,
        chapters: chapters
      };
    }
    
    // Estat√≠sticas
    const totalContentLength = bookData.chapters?.reduce((sum: number, chapter: any) => 
      sum + (chapter.content?.length || 0), 0) || 0;
    
    console.log('üìà ESTAT√çSTICAS:');
    console.log(`   ‚Ä¢ Modelo usado: ${model}`);
    console.log(`   ‚Ä¢ Cap√≠tulos: ${bookData.chapters?.length}`);
    console.log(`   ‚Ä¢ Caracteres: ${totalContentLength}`);
    console.log(`   ‚Ä¢ P√°ginas estimadas: ~${Math.ceil(totalContentLength / 1800)}`);
    
    // Metadados
    bookData.metadata = {
      model: model,
      maxTokens: maxTokens,
      size: adjustedConfig.label,
      estimatedPages: Math.ceil(totalContentLength / 1800),
      generation: "auto-detected-model"
    };
    
    return NextResponse.json(bookData);
    
  } catch (error: any) {
    console.error('üí• ERRO CR√çTICO:', error.message);
    
    if (error.message.includes('NENHUM modelo')) {
      return NextResponse.json(
        { 
          error: 'Projeto sem acesso a modelos GPT',
          solution: 'Verifique as permiss√µes do projeto na OpenAI ou use uma API Key diferente'
        },
        { status: 403 }
      );
    }
    
    if (error?.status === 429) {
      return NextResponse.json(
        { error: 'Limite de requisi√ß√µes excedido' },
        { status: 429 }
      );
    }
    
    return NextResponse.json(
      { 
        error: 'Erro na gera√ß√£o',
        details: error.message
      },
      { status: 500 }
    );
  }
}
